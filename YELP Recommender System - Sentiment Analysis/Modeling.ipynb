{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwP1E1R47iuTsDGwtzpgZl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aLJguFkxJWAI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jx0DW85OMGBz"},"source":["# **III. Supervised Machine Learning Predictive Modeling**\n","\n","Models include:\n","\n","1. Naive Bayes\n","\n","2. Support Vector Machine\n","\n","3. Logistic Regression\n","\n","4. Random Forest\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2dYtk3JZMjbA"},"source":["**1. Naive Bayes**"]},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\n","nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", weightCol=\"weight\")"],"metadata":{"id":"_VADfZHHhe6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer"],"metadata":{"id":"8b5S_EpFh8D-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stages = []\n","# 1. clean data and tokenize sentences using RegexTokenizer\n","regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n","stages += [regexTokenizer]\n","\n","# 2. CountVectorize the data\n","cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n","stages += [cv]\n","\n","# 3. Convert the labels to numerical values using binariser\n","indexer = StringIndexer(inputCol=\"label\", outputCol=\"label1\")\n","stages += [indexer]\n","\n","# 4. Vectorise features using vectorassembler\n","vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n","stages += [vecAssembler]\n","\n","[print('\\n', stage) for stage in stages]"],"metadata":{"id":"soCGqmN-h_u2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vJ1lGJ1E8Qk","outputId":"a67d83ff-beb3-475d-cc90-5aca659b6a4d","executionInfo":{"status":"ok","timestamp":1682417246175,"user_tz":300,"elapsed":10,"user":{"displayName":"Nithin Gowda","userId":"14807947994139678355"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," RegexTokenizer_5edf07f9deeb\n","\n"," CountVectorizer_eaacfaefa196\n","\n"," StringIndexer_ce825f6570e2\n","\n"," VectorAssembler_13f76b4c4a0c\n"]},{"output_type":"execute_result","data":{"text/plain":["[None, None, None, None]"]},"metadata":{},"execution_count":29}],"source":["stages = []\n","# 1. clean data and tokenize sentences using RegexTokenizer\n","regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n","stages += [regexTokenizer]\n","\n","# 2. CountVectorize the data\n","cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n","stages += [cv]\n","\n","# 3. Convert the labels to numerical values using binariser\n","indexer = StringIndexer(inputCol=\"label\", outputCol=\"label1\")\n","stages += [indexer]\n","\n","# 4. Vectorise features using vectorassembler\n","vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n","stages += [vecAssembler]\n","\n","[print('\\n', stage) for stage in stages]"]},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","pipeline = Pipeline(stages=stages)\n","data = pipeline.fit(tfidf_df).transform(tfidf_df)"],"metadata":{"id":"rsYlGhKViEfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_nb, test_nb = data.randomSplit([0.8, 0.2], seed = 2018)"],"metadata":{"id":"9J3TB6jxiHhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n","model_nb = nb.fit(train_nb)"],"metadata":{"id":"9T0-VliBiHkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"js7dLb2LIzlt","executionInfo":{"status":"ok","timestamp":1682422182671,"user_tz":300,"elapsed":2085982,"user":{"displayName":"Nithin Gowda","userId":"14807947994139678355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2ec2e33-4abb-4de8-b8f2-1925b1339869"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----------+-------------------------------------------+\n","|label|prediction|probability                                |\n","+-----+----------+-------------------------------------------+\n","|1    |1.0       |[4.5902986561809365E-11,0.9999999999540969]|\n","|1    |1.0       |[4.3455584210085304E-7,0.9999995654441579] |\n","|1    |1.0       |[5.9377322148630876E-8,0.9999999406226778] |\n","|0    |0.0       |[0.9969209935083592,0.0030790064916408834] |\n","|1    |1.0       |[0.002129835933854133,0.9978701640661459]  |\n","|1    |1.0       |[0.0030842835789838745,0.9969157164210161] |\n","|1    |1.0       |[2.638334185789453E-17,1.0]                |\n","|1    |1.0       |[7.22552172467018E-8,0.9999999277447827]   |\n","|1    |1.0       |[1.626316976449768E-13,0.9999999999998375] |\n","|0    |0.0       |[0.9999999999999987,1.379233476913504E-15] |\n","+-----+----------+-------------------------------------------+\n","\n"]}],"source":["predictions = model_nb.transform(test_nb)\n","# Select results to view\n","predictions.limit(10).select(\"label\", \"prediction\", \"probability\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikiVzq51LpOg","executionInfo":{"status":"ok","timestamp":1682423466989,"user_tz":300,"elapsed":1250183,"user":{"displayName":"Nithin Gowda","userId":"14807947994139678355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bee9c00e-00b9-47ba-9dee-f5cf20c8c0a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Area Under ROC:  0.8496871988138883\n"]}],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n","accuracy = evaluator.evaluate(predictions)\n","print (\"Test Area Under ROC: \", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"he4Fc942M6p-"},"source":["**2. Support Vector Machine**"]},{"cell_type":"code","source":["# SVM model\n","numIterations = 50\n","regParam = 0.3\n","svm = SVMWithSGD.train(train_lb, numIterations, regParam=regParam)\n","\n","# predict\n","test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n","scoreAndLabels_test = test_lb.map(lambda x: (float(svm.predict(x.features)), x.label))\n","score_label_test = spark.createDataFrame(scoreAndLabels_test, [\"prediction\", \"label\"])"],"metadata":{"id":"6es6SfHqje7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Model Evaluation\n","f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n","svm_f1 = f1_eval.evaluate(score_label_test)\n","print(\"F1 score: %.4f\" % svm_f1)"],"metadata":{"id":"q5m49wqXjjF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabulary = cvModel.vocabulary\n","weights = svm.weights.toArray()\n","svm_coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\n","\n","svm_coeffs_df.sort_values('weight').head(20)"],"metadata":{"id":"cMLTza1jjni8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"loyQvcd0M63C"},"source":["**3. Logistic Regression**"]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","\n","lr = LogisticRegression( labelCol='label', maxIter=10)\n","lrModel = lr.fit(train_nb)"],"metadata":{"id":"PDOFVR6RiZH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lrModel = lr.fit(train)\n","lr_pred = lrModel.transform(test)\n","f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n","lr_f1 = f1_eval.evaluate(lr_pred)\n","print(\"F1 score: %.4f\" % lr_f1)"],"metadata":{"id":"mVpwg6ofiaJU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y5DOV71FM69L"},"source":["**4. Random Forest**"]},{"cell_type":"code","source":["from pyspark.ml.classification import (DecisionTreeClassifier, RandomForestClassifier,\n","                                      GBTClassifier)\n","from pyspark.ml import Pipeline\n"],"metadata":{"id":"JzxqeqhmifOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a vector assembly\n","assembler = VectorAssembler(inputCols=['tfidf'], outputCol='features')\n","final_data = assembler.transform(tfidf_df)"],"metadata":{"id":"HgZllYHLihf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZdMHaSnARpk","executionInfo":{"status":"ok","timestamp":1682444073112,"user_tz":300,"elapsed":5,"user":{"displayName":"Nithin Gowda","userId":"14807947994139678355"}},"outputId":"50b6aa3e-4d07-4086-900c-46bb62f4bbe7"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- review_id: string (nullable = true)\n"," |-- text: string (nullable = true)\n"," |-- label: integer (nullable = true)\n"," |-- words: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- words_new: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- tf: vector (nullable = true)\n"," |-- tfidf: vector (nullable = true)\n"," |-- features: vector (nullable = true)\n","\n"]}],"source":["final_data.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93Eq06TTYvDt","executionInfo":{"status":"ok","timestamp":1682444145739,"user_tz":300,"elapsed":70658,"user":{"displayName":"Nithin Gowda","userId":"14807947994139678355"}},"outputId":"84071237-4dd7-44f5-d31f-d00c7ff2858e"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- review_id: string (nullable = true)\n"," |-- text: string (nullable = true)\n"," |-- label: integer (nullable = true)\n"," |-- words: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- words_new: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- tf: vector (nullable = true)\n"," |-- tfidf: vector (nullable = true)\n"," |-- features: vector (nullable = true)\n"," |-- label1: double (nullable = false)\n","\n"]}],"source":["from pyspark.ml.feature import StringIndexer\n","\n","indexer = StringIndexer(inputCol = 'label', outputCol = 'label1')\n","outputFixed = indexer.fit(final_data).transform(final_data)\n","outputFixed.printSchema()"]},{"cell_type":"code","source":["outputFixed.printSchema()\n","\n","final_df = outputFixed.select('features', 'label1')\n","final_df.show(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehleB4yjWjvv","executionInfo":{"status":"ok","timestamp":1682444681878,"user_tz":300,"elapsed":536144,"user":{"displayName":"Nithin Gowda","userId":"14807947994139678355"}},"outputId":"4bf8cb1a-6db5-4b25-ca86-3374ae43000f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- review_id: string (nullable = true)\n"," |-- text: string (nullable = true)\n"," |-- label: integer (nullable = true)\n"," |-- words: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- words_new: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- tf: vector (nullable = true)\n"," |-- tfidf: vector (nullable = true)\n"," |-- features: vector (nullable = true)\n"," |-- label1: double (nullable = false)\n","\n","+--------------------+------+\n","|            features|label1|\n","+--------------------+------+\n","|(202063,[0,1,2,6,...|   0.0|\n","|(202063,[0,7,14,1...|   0.0|\n","|(202063,[0,2,3,13...|   0.0|\n","+--------------------+------+\n","only showing top 3 rows\n","\n"]}]},{"cell_type":"code","source":["# Scaling the data\n","scaler = StandardScaler(inputCol='features',outputCol='scaledFeat')\n","final_data = scaler.fit(final_data).transform(final_data)"],"metadata":{"id":"8r7VKvLOi-Y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_rf, test_rf = final_df.randomSplit([0.8, 0.2])"],"metadata":{"id":"zYFxgOISi-bo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Building the random forest model\n","gb = GBTClassifier(labelCol = 'label1', featuresCol = 'scaledFeat')\n","gb_model = gb.fit(train)"],"metadata":{"id":"PLHToAPDi-eI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_predictions = rf_model.transform(test)"],"metadata":{"id":"gbnEFn7Ri-hI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","binary_evaluator = BinaryClassificationEvaluator(labelCol = 'label1')"],"metadata":{"id":"RPq9SsHai-ke"},"execution_count":null,"outputs":[]}]}